% !TEX root = ../main.tex

\chapter{\sysname 系统实现}

\section{总体实现概述}

\sysname 采用三层架构，自上而下分别是：应用接口层、链路执行器以及运行时。
其目标是在不暴露 RDMA 或 NVLink 的具体机制的前提下，为上层提供统一且稳定的通信语义。

图 ~\ref{fig:sender-recver-interface} 展示了应用接口层提供的抽象。
应用接口层向用户代码暴露了 \texttt{Sender} 与 \texttt{Recver} 纯虚类，它们是点对点通信操作逻辑通道的端口：
前者负责发起本端到远端的数据传输，后者负责在本端接收远端发送的数据。
对于应用而言，这两个接口提供一致的 \texttt{send()} / \texttt{recv()} 函数调用，
与底层链路是 RDMA 还是 NVLink 无关。

\begin{figure}[htbp]
    \centering
    \begin{minted}[
        frame=lines,
        linenos,
        fontsize=\small,
        numbersep=10pt,
        framesep=3mm
    ]{cpp}
class Sender {
public:
  virtual shared_ptr<Event> send(uint32_t unique_id, uint64_t addr,
                                 uint64_t length) = 0;
  virtual ~Sender() = default;
};

class Recver {
public:
  virtual shared_ptr<Event> recv(uint32_t unique_id, uint64_t addr,
                                 uint64_t length) = 0;
  virtual ~Recver() = default;
};
    \end{minted}
    \caption{Sender 与 Recver 接口定义}
    \label{fig:sender-recver-interface}
\end{figure}

在不同的传输链路下， \texttt{Sender} 与 \texttt{Recver} 分别对应不同类型的执行器实现：
RDMA 场景对应 \texttt{RdmaSender} 与 \texttt{RdmaRecver}，
NVLink 场景对应 \texttt{NvlinkSender} 与 \texttt{NvlinkRecver}。
但执行器的内部逻辑（例如请求匹配、CUDA IPC Handle 以及地址偏移量交换、控制面队列的使用方式等）均对应用隐藏，
应用只需提供唯一标识符（\texttt{unique\_id}）、缓冲区地址以及长度，即可提交一次完整的发送或接收请求。

所有发送与接收操作均返回一个 \texttt{Event} 事件对象指针用于异步完成通知。
应用在提交请求后立即获得对应事件，可选择同步阻塞等待或是异步检查完成 Event 对象状态的方式，来确认请求是否完成。
事件何时被触发、由链路执行器在什么条件下调用 \texttt{notify()} 等细节会在后续章节进行讨论，不在接口层暴露。

通过这一轻量化接口设计，\sysname 将应用侧从底层通信机制中彻底解耦，
使得上层推理框架只需按照统一语义调用 \texttt{send()} / \texttt{recv()}，
而无需感知实际的数据传输方式、链路差异或控制面结构。
接口层本身保持稳定，而具体执行路径则在后续章节中根据不同链路类型展开分析。

图 ~\ref{fig:app-send-recv-example} 展示了应用侧通过统一接口提交发送与接收请求的示例，
后台运行时负责驱动具体的链路执行器完成数据传输任务，
前台应用只需在返回的时间对象上调用 \texttt{wait()} 就可以显式同步等待请求完成。

\begin{figure}[htbp]
    \centering
    \begin{minted}[
        frame=lines,
        linenos,
        fontsize=\small,
        numbersep=10pt,
        framesep=3mm
    ]{cpp}
// Start background runtime
auto runtime_handle = /* ... */;
// Sender side
auto send_event = sender->send(id, send_buffer, size);
send_event->wait();
// Receiver side
auto recv_event = recver->recv(id, recv_buffer, size);
recv_event->wait();
    \end{minted}
    \caption{应用侧发送与接收调用示例}
    \label{fig:app-send-recv-example}
\end{figure}

这一统一接口层为随后的运行时实现、事件机制以及 RDMA / NVLink 执行器的具体逻辑打下基础。

\section{运行时实现}

\sysname 的运行时层负责在统一接口与具体链路执行器之间建立稳定且独立的调度环境。
相比于底层 RDMA / NVLink 执行器中承担的数据面与控制面逻辑，运行时本身不参与传输细节的管理，
而是专注于为执行器提供一致的轮询调用、线程上下文以及必要的资源隔离，从而保证各类执行器能够在统一框架下协同工作。

\begin{figure}[htbp]
    \centering
    \begin{minted}[
        frame=lines,
        linenos,
        fontsize=\small,
        numbersep=10pt,
        framesep=3mm
    ]{cpp}
auto pollables = /* pollable sender and recver objects */;
auto runtime = RuntimeBuilder()
                    .with_pollables(std::move(pollables))
                    .with_num_threads(4)
                    .with_affinity(cpu_set)
                    .build()
                    .unwrap();
auto handle = runtime->start();
    \end{minted}
    \caption{运行时初始化流程}
    \label{fig:runtime-init}
\end{figure}

图 ~\ref{fig:runtime-init} 展示了运行时的初始化和启动流程。
其构造过程由 \texttt{RuntimeBuilder} 构造器完成。
构造器收集用户配置，包括可驱动的对象列表、线程数量、CPU 亲和性以及轮询间隔等，
并调用工厂方法 \texttt{build()} 构造具体的 \texttt{Runtime} 对象。
该对象在启动后负责创建后台线程池，并按照预先绑定的方式将 Sender 与 Recver 以及其他可驱动对象分配给固定线程。

运行时并不关心执行器的内部协议或链路类型，只要求其符合可驱动对象的接口定义（如图 ~\ref{fig:pollable-interface}），
并且其 \texttt{poll()} 方法具有单线程推进语义，
绑定固定线程策略保证每个执行器的 \texttt{poll()} 只会在单一线程中被调用，确保线程安全。

\begin{figure}[htbp]
    \centering
    \begin{minted}[
        frame=lines,
        linenos,
        fontsize=\small,
        numbersep=10pt,
        framesep=3mm
    ]{cpp}
class Pollable {
public:
  virtual void poll() = 0;
  virtual ~Pollable() = default;
};
    \end{minted}
    \caption{可驱动对象接口定义}
    \label{fig:pollable-interface}
\end{figure}


在启动阶段，运行时根据构造器提供的 CPU 集合参数为每个后台线程设置线程亲和性。
对于跨涉及网卡和跨 NUMA 内存的执行器，这一策略可以避免执行器在轮询时发生跨 NUMA 节点的内存访问，
而在 NVLink 场景下则有助于保持后台拷贝线程与前台应用线程之间的调度稳定性。
线程启动后进入标准化的轮询循环，根据线程本地列表依次调用绑定的执行器的 \texttt{poll()}，
并根据用户配置决定是否在每轮轮询后进行短暂休眠。
轮询循环运行时层面保持统一，而具体的状态推进逻辑则由执行器自行实现。

可配置的 \texttt{poll\_interval} 为运行时在低延迟与低 CPU 占用之间提供了控制手段。
非零间隔会令线程在两次轮询间隔中进行短时间的休眠，使得运行时更适合低负载与多租户环境；
间隔为零时，线程采取轻量级忙轮询策略，能够最大化执行器的响应速度。
由于运行时不涉及链路内部操作，这一策略在不同类型执行器间具有一致的效果。

运行时的职责可以概括为三个方面：
第一，为所有链路执行器提供统一、隔离且无锁的轮询环境；
第二，通过线程绑定与亲和性控制确保执行器的 \texttt{poll()} 在稳定的线程上下文中运行；
第三，为上层接口层提供一个可靠的调度基础，使通信请求能够在后台持续推进。
数据传输、请求匹配、可见性保证等链路相关逻辑均由后续第~\ref{sec:rdma-executor-impl}~节与第~\ref{sec:nvlink-executor-impl}~节介绍的 RDMA / NVLink 执行器负责。

通过这样的设计，运行时为 \sysname 提供了一个与链路类型无关的调度基础，
使 RDMA 与 NVLink 执行器能够在相同的模型下运行，同时保持系统整体结构的模块化与可扩展性。

\section{控制面事件机制}

\sysname 引入异步事件（Event）机制，使控制面操作的提交与完成检测彻底解耦。
应用层在发起发送或接收请求后，会立即获得一个轻量级事件对象（\texttt{Event}），并可在之后通过显式等待或周期性轮询查询其状态。
该事件机制基于 C++20 原子变量及其等待/通知原语实现，避免了传统条件变量所需的内核态切换，
从而在多线程环境中提供更高效且可扩展的同步能力。

\begin{figure}[htbp]
    \centering
    \begin{minted}[
        frame=lines,
        linenos,
        fontsize=\small,
        numbersep=10pt,
        framesep=3mm
    ]{cpp}
class Event {
private:
  std::atomic<bool> finished_ = false;

public:
  bool is_notified() const {
    return this->finished_.load(std::memory_order_acquire);
  }

  void notify() {
    this->finished_.store(true, std::memory_order_release);
    if (!globalConfig.enable_busy_wait) {
      this->finished_.notify_all();
    }
  }

  void wait() {
    if (globalConfig.enable_busy_wait) {
      while (!this->finished_.load(std::memory_order_acquire)) {
        _mm_pause();
      }
    } else {
      this->finished_.wait(false, std::memory_order_acquire);
    }
  }
};
    \end{minted}
    \caption{事件类核心结构和方法}
    \label{fig:event-class}
\end{figure}

事件对象内部仅维护一个原子布尔值 \texttt{finished\_}，用于记录事件是否已完成。
围绕这一标志位，\sysname 提供了两种等待策略，可针对不同应用场景灵活选用。

第一种是阻塞式等待（blocking wait）。在支持 C++20 原子等待特性的系统中，
\texttt{wait()} 通过调用 \texttt{atomic::wait()} 在用户态阻塞，直到 \texttt{finished\_} 从 \texttt{false} 变为 \texttt{true}。
该方式不消耗 CPU 时间片，适用于事件完成时间难以预估、线程不希望持续占用处理器资源的场景。
事件完成后，生产者线程将标志位置为 \texttt{true}，并调用 \texttt{notify\_all()} 唤醒所有等待中的消费者线程，从而实现高效且无伪唤醒的同步。

第二种是忙等待（busy wait）模式。在编译期启用 \texttt{BUSY\_WAIT} 宏时，\texttt{wait()} 采用自旋方式，
通过不断轮询 \texttt{finished\_} 的值并执行 \texttt{\_mm\_pause()} 来降低总线竞争和功耗。
这种方式避免了阻塞所带来的调度延迟，适合对延迟极为敏感、且事件预计会在极短时间内完成的场景。通过 acquire/release 内存序，
消费者在检测到事件完成后能够正确观察到生产者线程对共享状态的所有更新。

通过同时提供阻塞式等待与忙等待两种策略，\sysname 可以在 CPU 利用率与最短延迟之间取得灵活平衡，
使系统在不同硬件和负载条件下均能保持高效运行。图 ~\ref{fig:event-class} 展示了事件类的核心结构。

\section{应用接口和链路执行器封装}

在统一接口层的设计中，\sysname 采用轻量的 \texttt{Sender} 与 \texttt{Recver} 两类封装对象，
将 RDMA 与 NVLink 两种链路下的执行器实现隐藏在一致的抽象之后，使得应用在调用 \texttt{send()} 与 \texttt{recv()} 时完全无需关注底层链路的差异。
接口层本身不包含任何传输逻辑，只负责对用户的请求进行封装和转发，调用具体的链路执行器实现进行处理。
具体的链路执行器也继承了 \texttt{Pollable} 并重写了 \texttt{poll()} 方法，
运行时轮询阶段调用具体链路执行器对象的 \texttt{poll()} 方法时，
通过 C++ 的动态多态分派到对应的底层实现。

从应用视角来看，所有的发送流程都遵循固定语义：
在给定标识符（\texttt{unique\_id}）、起始地址与长度后，接口层立即为这个传输构造一个 \texttt{Event} 并返回。
之后的所有工作均转交给链路执行器，包括请求的匹配、调度推进与最终的完成通知。
在接收方向，\texttt{recv()} 采用完全对称的方式构造请求对象并交付底层处理。

内部实现中，采用了基于 moodycamel 的并发多生产者多消费者队列（ConcurrentQueue）作为接口层与执行器之间的消息传递数据结构。
对于 RDMA 场景，该队列承载由应用层 \texttt{send()} / \texttt{recv()} 产生的本地请求，
并交由执行器在 \texttt{poll()} 阶段取出，实现严格的单线程状态推进语义；
对于 NVLink 场景，队列则承担本地命令缓存角色，与来自进程间共享内存 SPSC 队列的远端命令一并进入匹配逻辑。
ConcurrentQueue 的无锁特性保证了接口层的请求提交端完全无阻塞，不会因后台执行器的处理速度而影响应用层的前向执行通道；
同时，其基于 C++ 原子语义的实现确保不同线程间的生产-消费关系具有清晰的内存可见性边界，
避免出现乱序加载导致的请求丢失或重复消费。

% 接口层的另一个关键工作是维护内存访问凭证的查询功能。
% 在 RDMA 场景下，发送者需要从 MemoryRegion 中查询本地地址对应的 lkey，
% 而接收者需要查询接收缓冲区对应的 rkey，内存访问凭证是 lkey 和 rkey；
% 而在 NVLink 场景下，执行器会利用 CUDA IPC 机制获取到的 \texttt{cudaIpcMemHandle\_t} 进行跨进程的内存拷贝，
% 内存访问凭证是 \texttt{cudaIpcMemHandle\_t} 。
% 因此接口层使用的 \texttt{lookup\_key()} 尽管名称相同，但在不同后端会直接分派至不同的实现路径。
% 通过这种统一接口 + 手动分派的策略，\sysname 在保证高性能的前提下实现了透明的跨链路语义。

接口层与运行时之间的边界十分清晰。运行时不会干预 \texttt{send()} / \texttt{recv()} 的逻辑构造，
也不关心底层使用的是 RDMA libibverbs 还是通过 NVLink 的进行跨 GPU 的内存拷贝。
运行时仅负责保证 \texttt{poll()} 方法在绑定线程上按固定频率执行，
接口层则在 \texttt{poll()} 中调用底层执行器的对应方法，使得真正的数据面与控制面推进逻辑在执行器内部完成，
而接口层始终保持无状态、轻量级的请求入口角色。

接口层最终形成了一个稳定且扩展性良好的抽象边界：
它既屏蔽了 RDMA 与 NVLink 在队列结构、匹配策略、可见性保证上的巨大差异，
又通过事件机制与运行时轮询模型提供统一的调度语义，使得通信库整体可以在异构网络环境中维持一致的逻辑行为。
这一接口层是 \sysname 全局结构中轻量级的一层，但是它可以将应用、运行时与执行器串联在一起，发挥了重要的作用。

\section{RDMA 链路执行器实现}
\label{sec:rdma-executor-impl}

\subsection{初始化阶段}

RDMA 链路执行器的实现分为两层：
底层的 \texttt{rdma\_util} 封装 \texttt{libibverbs} 接口并以 RAII 管理 RDMA 资源，
上层的 \texttt{executor\_rdma} 则利用这些抽象构建发送端 \texttt{RdmaSender}、
接收端 \texttt{RdmaRecver} 以及维护 GPU Direct RDMA 一致性的 \texttt{RdmaFlusher}。
初始化阶段的核心任务包括：创建 RDMA 基础资源、完成 QP 握手、分配控制面缓冲区及 MR 注册。

\paragraph{RDMA 基础资源创建}

\texttt{rdma\_util} 封装了 RDMA 资源的创建与管理，
核心包括 \texttt{Context}、\texttt{ProtectionDomain}、
\texttt{CompletionQueue}以及 \texttt{RcQueuePair}。
这些类均通过静态工厂方法进行构造，返回智能指针以管理生命周期，
并在析构函数中负责释放底层资源，确保 RAII 语义。

执行过程如下：

\begin{enumerate}
    \item 获取设备上下文：\texttt{Context} 内部调用 \texttt{ibv\_get\_device\_list} 查找用户指定的 RDMA 设备，
          再使用 \texttt{ibv\_open\_device} 打开对应的 \texttt{ibv\_context}。
    \item 保护域与完成队列对创建：\texttt{ProtectionDomain} 使用 \texttt{ibv\_alloc\_pd} 构造，
          \texttt{CompletionQueue} 使用 \texttt{ibv\_create\_cq} 创建发送 CQ 和接收 CQ。
    \item QP 创建：\texttt{RcQueuePair::create()} 将上述资源组合生成可靠连接类型队列对（Reliable Connected QP），
          并自动负责析构阶段的资源回收。
\end{enumerate}

\begin{figure}[htbp]
    \centering
    \begin{minted}[
        frame=lines,
        linenos,
        fontsize=\small,
        numbersep=10pt,
        framesep=3mm
    ]{cpp}
template <typename T>
using Result = result::Result<T, rdma_util::Error>;

class RcQueuePair {
public:
  // Create RC QP with different inputs
  static Result<unique_ptr<RcQueuePair>> create(/*...*/) noexcept;
  // Used to establish connection
  Result<HandshakeData> get_handshake_data(/*...*/) noexcept;
  Result<void> bring_up(/*...*/) noexcept;
  // RDMA opreations
  void fill_wr(ibv_send_wr&) noexcept;
  int post_send_wrs(ibv_send_wr* wrs) noexcept;
  int post_recv_wrs(ibv_recv_wr* wrs) noexcept;
  int poll_send_cq_once(/*...*/) noexcept;
  int poll_recv_cq_once(/*...*/) noexcept;
};
    \end{minted}
    \caption{RDMA 资源初始化}
    \label{fig:rdma-init}
\end{figure}

\paragraph{QP 握手与状态迁移}

可靠连接队列对（RCQP）要进入可通信状态，必须经历 INIT $\rightarrow$ RTR $\rightarrow$ RTS 的三段状态迁移，
由于这一步要求通信双方根据对端配置进行状态修改，因此也被称为“握手”。
\texttt{rdma\_util} 通过通信双方的 QP 交换 \texttt{HandshakeData}
并且显式调用 \texttt{RcQueuePair::bring\_up()} 完成这一握手过程，
传输 \texttt{HandshakeData} 结构需要通过带外信道完成，
\sysname 提供了一套简单了基于 TCP 的远程过程调用（RPC）框架，支持在初始化阶段交换该结构。

\texttt{bring\_up()} 会使用 \texttt{ibv\_modify\_qp} 完成所有修改，并在必要时查询当前状态避免错误重复配置。
\texttt{bring\_up()} 的具体步骤如下：

\begin{enumerate}
    \item 查询并确认当前 QP 状态，若为 \texttt{RTS} 则直接返回，否则要求状态为 \texttt{RESET}；
    \item 将 QP 修改为 \texttt{INIT}，设置端口号、PKey 索引以及访问权限（本地写、远程读、远程写）；
    \item 将 QP 修改为 \texttt{RTR}，填入对端的 QP number、LID、GID，配置地址向量（含 GRH 字段和 \texttt{sgid\_index}），并设置 MTU、接收 PSN、\texttt{max\_dest\_rd\_atomic} 和 \texttt{min\_rnr\_timer}；
    \item 将 QP 修改为 \texttt{RTS}，设置发送 PSN、本端支持的最大 RDMA 读并发数以及超时与重试参数（\texttt{timeout}、\texttt{retry\_cnt}、\texttt{rnr\_retry}）。
\end{enumerate}

\paragraph{控制面缓冲区与 MR 注册}

实际 RDMA 数据面 buffer 通常由应用层提供，执行器内部只注册控制面需要的 host memory。由于控制面消息类型固定（\texttt{RdmaTicket}），
\texttt{RdmaSender} 与 \texttt{RdmaRecver} 均会分配固定大小的数组作为控制面消息的缓冲区，并将其注册为 MR，用于进行高性能控制面通信。
以下代码展示了 MR 的创建过程：

\begin{figure}[htbp]
    \centering
    \begin{minted}[
        frame=lines,
        linenos,
        fontsize=\small,
        numbersep=10pt,
        framesep=3mm
    ]{cpp}
auto pd = /* Create Protection Domain */;
auto size = sizeof(RdmaTicket) * kMagic;
auto buf = shared_ptr<void>(std::aligned_alloc(64, size), std::free);
auto mr = MemoryRegion::create(pd, std::move(buf), size).unwrap();
// After that, buf can be used for RDMA operations
    \end{minted}
    \caption{内存区域（MR）创建}
    \label{fig:memory-region-setup}
\end{figure}

\paragraph{RdmaFlusher 实现}

GPU Direct RDMA 的完成语义需要额外区分“网卡侧完成”与“设备侧可见”：
RDMA Write with Imm 在 CPU 侧产生 CQE 仅说明该写入请求在 RNIC 侧完成，
但其通过 PCIe DMA 写入 GPU 显存后的可见性仍可能受写合并、乱序与设备侧缓存行为等因素影响。
因此，若应用直接将 CQE 作为“数据可消费”的判据，则在极端情况下可能观察到旧值或不完整数据。

为此，\sysname 在接收端引入 \texttt{RdmaFlusher}，通过额外插入一条后继 RDMA Read 建立必要的顺序关系：
RDMA 链路执行器在初始化阶段基于环回 QP 构造了一个 Flusher，该 QP 的发送与接收都指向自己；
接收端在检测到“最后一个分片对应的 RDMA Write with Imm 完成”后，不立即触发事件完成，
而是将该请求交由 Flusher 处理；
Flusher 对同一 GPU 内存区域发起一次小规模 RDMA Read，
并仅在该 Read 完成后再调用 \texttt{Event::notify()}。

利用 PCIe/RDMA 路径上的顺序保证，后继 Read 等价于一道控制面“栅栏”，
PCIe 会保证所有先前写入的数据在此 READ 之后对 GPU 可见，从而为上层提供一致的完成语义；
并可通过批量处理或仅对最后分片执行 Flush 等策略来减少额外的控制开销。
利用这种读-写屏障方法可以有效提供一致的完成语义，而无需对现有网络协议和硬件做更改 

\begin{figure}[htbp]
    \centering
    \begin{minted}[
        frame=lines,
        linenos,
        fontsize=\small,
        numbersep=10pt,
        framesep=3mm
    ]{cpp}
// Create Reliable Connection QP
auto qp = RcQueuePair::create(pd).unwrap();
// Connect QP to itself
qp->bring_up(qp->get_handshake_data().unwrap()).unwrap();
// Create RDMA Flusher
auto flusher = RdmaFlusher::create(std::move(qp)).unwrap();
    \end{minted}
    \caption{Flusher 创建流程}
    \label{fig:flusher-setup}
\end{figure}

\subsection{运行阶段}

在运行阶段，RDMA 链路执行器由三个核心组件
发送侧 \texttt{RdmaSender}、接收侧 \texttt{RdmaRecver} 与 \texttt{RdmaFlusher} 协同工作，
由核心运行时线程池以绑定式线程模型在独立线程中推进。

发送端 \texttt{RdmaSender} 的运行主逻辑封装在 \texttt{poll()} 中，通过依次调用四个内部函数推进状态：

% \begin{figure}[htbp]
%     \centering
%     \begin{minted}[
%         frame=lines,
%         linenos,
%         fontsize=\small,
%         numbersep=10pt,
%         framesep=3mm
%     ]{cpp}
% void RdmaSender::poll() {
%   this->drain_remote_recv_requests();
%   this->drain_local_send_requests();
%   this->build_and_post();
%   this->handle_completions();
% }
%     \end{minted}
%     \caption{RdmaSender 运行主逻辑}
%     \label{fig:rdma-sender-poll}
% \end{figure}

\paragraph{\texttt{drain\_remote\_recv\_requests()}}
负责从 RDMA Recv 队列取出由接收端发来的 \texttt{RdmaTicket}，
并将其整理为按 \texttt{unique\_id} 逻辑通道分组的待匹配队列。具体流程如下：

首先调用 \texttt{poll\_recv\_cq\_once()} 批量轮询接收完成事件，然后对每个成功的 RECV，
从接收缓冲区中拷贝出对应的 \texttt{RdmaTicket}，
按照 \texttt{unique\_id} 为键插入到 \texttt{pending\_remote\_recv\_request\_map\_} 中，为后续的匹配与发送做准备，
同时立即向网卡 Recv 队列重新投递 RECV 工作请求，保持网卡始终有充足槽位可以使用，在不阻塞 CQ 的前提下高效处理控制面请求。

\paragraph{\texttt{drain\_local\_send\_requests()}}
用于从应用接口层的并发队列中取出本地发送请求，也将其转换为按 \texttt{unique\_id} 逻辑通道分组的待发送队列。
应用侧调用 \texttt{send()} 时，会将包含 \texttt{RdmaTicket} 与对应的 \texttt{Event} 的 \texttt{RdmaCommand} 入队到并发队列中；
在运行时线程中，\texttt{drain\_local\_send\_requests()} 从并发队列的另一端批量出队多个命令，
将其中的 \texttt{ticket} 和 \texttt{event} 按照 \texttt{unique\_id} 保存，
这样可以在不加锁的情况下，将无序到达的应用发送请求归类到多个逻辑通道，为后续匹配和构造 WR 构造提供基础。

\paragraph{\texttt{build\_and\_post()}} 职责是在发送端维护的控制面队列中执行请求匹配，
并将配对后的传输任务按照最大分片大小拆分为一系列连续的工作请求（Work Request，WR），最终以批量方式提交给网卡。
其核心流程由三部分构成：跨队列匹配、分片生成与批量门铃触发。
算法 \ref{alg:rdma-build-and-post} 展示了请求匹配和发送的核心逻辑。

首先逐一遍历所有 \texttt{unique\_id} 对应的远端接收队列与本地发送队列,当且仅当两端均存在待处理的请求，
且发送 WR 数组仍有剩余槽位时，算法取出两端的请求票据 \texttt{RdmaTicket} 并进入匹配过程。
由于每一次传输的总长度由票据给定，函数据此确定当前尚未完成的剩余字节数。

随后进入分片循环，若剩余字节数大于最大分片大小，
则生成长度为最大分片大小的 RDMA Write 工作请求，并且标识请求尚未完成；
否则生成长度为剩余字节数的 RDMA Write with Imm 工作请求，把当前请求的逻辑通道标识符通过立即数传递给对端，
并在其 wr id 中编码中标记为这个待处理请求的最后一个分片。
两类 WR 的生成逻辑统一抽象为 \texttt{fill\_wr()}，其内部负责写入本地与远端地址、长度、键值以及标识符等必要元信息。
每构造一条新的 WR，都将其链接到上一条 WR 之后，从而形成一个连续的链表以便批量提交。
同时，算法维护当前已累积的 WR 链表长度，用于确定何时触发信号。

当 WR 属于某个待处理请求的最后一个分片，或 WR 数组空间已用尽时，该 WR 被设置为“需要显式信号”。
显示信号确保网卡在处理完这批 WR 之后，在本地网卡的发送完成队列（send completion queue）生成一次工作完成（Work Completion，WC）。
每次设置显式信号 WR，当前 WR 链表长度被重置为零，表示一个批次的结束。

分片循环结束后，若对应的远端和本地请求已全部完成，则从队列中弹出；
若仍有剩余，则更新票据的地址与剩余长度，等待下一轮构造。

当本轮至少构造出一条 WR 时，算法以链表首元素作为入口，
仅通过一次提交操作即可将整个 WR 链表交付给网卡，由硬件以链表顺序处理。
这种批处理显著减少了 MMIO ，是高性能 RDMA 发送端的常见优化方式。

\begin{algorithm}[t]
    \caption{RdmaSender 请求匹配和批处理算法}
    \label{alg:rdma-build-and-post}
    \KwIn{
        map of remote recv queues $\mathit{Remote}[id]$;
        map of local send queues $\mathit{Local}[id]$;
        current number of occupied WR slots \texttt{wr\_occupied};
        total WR capacity $N$;
        maximum chunk size \texttt{packet\_size}
    }
    \KwOut{append a contiguous WR chain to the WR array and post it to the NIC}

    \BlankLine
    $wr\_start \gets wr\_occupied$ \;
    $doorbell\_len \gets 0$ \;

    \ForEach{$(id, RemoteQueue) \in \mathit{Remote}$}{
        $LocalQueue \gets \mathit{Local}[id]$ \;

        \While{
            $wr\_occupied < N$ \textbf{and}
            $RemoteQueue$ not empty \textbf{and}
            $LocalQueue$ not empty
        }{
            $doorbell\_len \gets doorbell\_len + 1$ \;

            $remote,\, local \gets RemoteQueue.\text{front}(),\, LocalQueue.\text{front}()$ \;

            $len \gets local.length$ \;
            $chunk\_size \gets \min(\texttt{packet\_size},\ len)$ \;

            \eIf{$chunk\_size = len$}{
                $opcode,\, request\_finished \gets \texttt{WRITE\_WITH\_IMM},\, \text{true}$ \;
            }{
                $opcode,\, request\_finished \gets \texttt{WRITE},\, \text{false}$ \;
            }

            $wr\_idx,\, wr\_occupied \gets wr\_occupied,\, wr\_occupied + 1$ \;

            \texttt{fill\_wr}$(wr\_idx,\ id,\ remote,\ local,\ chunk\_size,\ opcode,\ doorbell\_len)$ \;

            \If{$request\_finished$}{
                $RemoteQueue.\text{pop}()$ \;
                $LocalQueue.\text{pop}()$ \;
            }
            \Else{
                $local.\text{advance}(chunk\_size) $ \;
                $remote.\text{advance}(chunk\_size) $ \;
                $RemoteQueue.\text{front}(),\, LocalQueue.\text{front}() \gets remote,\, local$ \;
            }

            \If{$opcode = \texttt{WRITE\_WITH\_IMM}$ \textbf{or} $wr\_occupied = N$}{
                \texttt{mark\_signaled}$(wr\_idx)$ \;
                $doorbell\_len \gets 0$ \;
            }
        }
    }

    \BlankLine
    \If{$wr\_occupied > wr\_start$}{
        \texttt{post\_send}(\text{WR at index } $wr\_start$ as the head of the chain) \;
    }
\end{algorithm}

\paragraph{\texttt{handle\_completions()}} 通过 \texttt{poll\_send\_cq\_once()} 轮询发送完成队列，
检查完成事件并进行处理。

每个成功的工作完成（work completion）都带有一个 64 位工作请求标识符（wr id），
需要对其解码，图 ~\ref{fig:wrid-encoding} 展示了编码格式。
该标识符由三部分组成：低 32 位为逻辑通道标识符 \texttt{unique\_id}，中间 16 位 \texttt{length} 为批处理提交的 WR 数量，
\texttt{finished} 标志占据 1 位，指示该 WR 是否为某个请求的最后一个分片。

首先解析出本次门铃中包含的 WR 数量，从而释放已占用的 WR 进行重用；
其次如果 \texttt{finished} 位为真，表示逻辑通道标识符为 \texttt{unique\_id} 的请求的最后一个分片已经发送完成，
从 \texttt{pending\_local\_send\_event\_map\_} 中取出对应事件并调用
\texttt{notify()} 告知上层应用发送完成。
这样，应用层看到的发送完成语义始终对应于“整个请求的最后一个分片发送完成”，有效简化了上层使用难度。

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.6\linewidth]{figures/wrid_encoding.png}
    \caption{工作请求标识符编码示意图}
    \label{fig:wrid-encoding}
\end{figure}

\paragraph{\texttt{RdmaRecver::poll()}} 接收端 \texttt{poll()} 的运行逻辑与发送端对称，
其核心目标是在控制面与数据面之间推进接收路径的状态机。
该过程可分为三个阶段，对应算法~\ref{alg:rdma-recver-poll} 所示的批量命令提取、接收缓冲区发布以及完成事件处理。

首先算法会尝试从并发队列中批量获取最多 \texttt{kMagic} 条命令，
每条命令均携带一个 \texttt{RdmaTicket} 以及其关联的 \texttt{Event}。
执行器将这个 ticket 按到达顺序插入待处理队列，用于后续 WR 槽位的顺序消耗；
同时按照逻辑通道标识符 \texttt{unique\_id}
将命令插入分组映射 \texttt{pending\_local\_recv\_request\_map\_}，
从而在后续根据立即数索引特定命令时能够实现常数时间的查找。此阶段的设计使接收端能够以批处理方式处理上层请求，
从而降低队列同步开销并提高流水线推进效率。

然后根据 WR 槽位资源的情况进行请求发送和信号接收，
只要存在空闲的 WR 槽位并且仍有待处理票据，算法就会进行如下操作：
\begin{enumerate}
    \item 从待处理队列中取出一条 ticket，将其序列化写入控制面发送缓冲区；
    \item 获取一个空闲的工作请求槽位；
    \item 在空闲槽位发布接收工作请求（Recv WR），用于接收发送端 RDMA Write with Imm 操作的立即数数据；
    \item 在空闲槽位发布发送工作请求（Send WR），将该 ticket 作为控制面消息发送给发送端。
\end{enumerate}
通过同时发布这两类 WR，接收端能够在数据面尚未到达之前，就准备好控制面信号缓冲区。

最后算法通过完成队列的轮询推进状态转换：
接收端首先检查所有控制面发送操作是否已成功完成，确保控制消息已被发送端收到；
随后从接收队列中批量获取 RDMA Write with Imm 类型的完成事件，每条事件都携带发送端写入的立即数，
因此执行器能够直接根据该值在 \texttt{pending\_local\_recv\_request\_map\_} 中检索对应的命令队列，并从中弹出队首命令。
随着 WR 槽位被回收至 \texttt{free\_slots}，一次接收操作完成。

完成语义的触发则取决于是否启用了 \texttt{RdmaFlusher}。
若未启用，算法会立即对完成的请求携带的事件执行 \texttt{notify()}，将命令完成同步给应用层；
若启用了 \texttt{RdmaFlusher}，则将 RDMA 写入的地址与键值交由其处理，
以通过额外的 RDMA Read 操作确保 GPU Direct RDMA 的可见性。
通过将“网络写完成”与“GPU 可见”两个语义显式解耦，该设计使接收端能够在不同硬件环境中保持一致的控制面行为，
同时在具备 GPU 设备内存的场景下提供强一致的数据可见性保障。

综上所述，\texttt{RdmaRecver::poll()}，通过批量处理命令、协调控制面发布以及分阶段处理完成事件，
实现了高效、稳健且可扩展的 RDMA 接收路径推进机制。

\begin{algorithm}[t]
    \caption{RdmaRecver 轮询算法}
    \label{alg:rdma-recver-poll}
    \KwIn{
        lock-free command queue $\mathit{CmdQueue}$ carrying \texttt{RdmaCommand};
        FIFO of pending tickets $\mathit{PendingFIFO}$;
        map from \texttt{unique\_id} to FIFO of pending commands $\mathit{PendingMap}[id]$;
        queue of free WR identifiers $\mathit{FreeSlots}$;
        maximum batch size $\texttt{kMagic}$;
        queue pair object \texttt{qp};
        optional flusher object \texttt{flusher}
    }
    \KwOut{publish new recv buffers and deliver completed events}

    \BlankLine
    \For{$i \gets 0$ \KwTo $\texttt{kMagic} - 1$}{
        \If{$\mathit{CmdQueue}.\text{try\_dequeue}(command)$}{
            $\mathit{PendingFIFO}.\text{push}(command.ticket)$ \;
            $\mathit{PendingMap}[command.ticket.unique\_id].\text{push}(command)$ \;
        }
        \Else{
            \textbf{break} \;
        }
    }

    \BlankLine
    \While{$\mathit{FreeSlots}$ not empty \textbf{and} $\mathit{PendingFIFO}$ not empty}{
        $ticket \gets \mathit{PendingFIFO}.\text{pop\_front}()$ \;
        $wr\_id \gets \mathit{FreeSlots}.\text{pop\_front}()$ \;
        \texttt{qp.post\_recv}$(wr\_id,\ \text{recv\_buffer\_slot}(wr\_id))$ \;
        \texttt{qp.post\_send\_send}$(wr\_id,\ \text{send\_buffer\_slot}(wr\_id))$ \;
    }

    \BlankLine
    \texttt{qp.poll\_send\_cq\_once}(\texttt{kMagic},\ send\_completions) \;

    \BlankLine
    \texttt{qp.poll\_recv\_cq\_once}(\texttt{kMagic},\ recv\_completions) \;
    \ForEach{$wc \in recv\_completions$}{
        $id \gets wc.imm\_data$ \;
        $wr\_id \gets wc.wr\_id$ \;
        $queue \gets \mathit{PendingMap}[id]$ \;
        $cmd \gets queue.\text{pop\_front}()$ \;
        $\mathit{FreeSlots}.\text{push}(wr\_id)$ \;

        \eIf{\texttt{flusher} is \textbf{null}}{
            $cmd.event.\text{notify}()$ \;
        }{
            \texttt{flusher.handle}$(cmd.ticket,\ cmd.event)$ \;
        }
    }
\end{algorithm}

\section{NVLink 链路执行器实现}
\label{sec:nvlink-executor-impl}

与前文的 RDMA 链路不同，同一节点内部通过 NVLink 连接的多 GPU 之间不存在网卡、QP 与 Completion Queue 等传统 RDMA 抽象，
\sysname 在该场景下将数据面下沉到 GPU Copy Engine，由后台拷贝线程统一调度，
控制面则通过进程间共享内存实现单生产者单消费者队列传递轻量控制面元数据。
executor\_nvlink 模块由发送端 \texttt{NvlinkSender}、接收端 \texttt{NvlinkRecver}
以及绑定到目标 GPU 的拷贝线程 \texttt{NvlinkCopier} 共同构成，
整体仍然遵循统一的 \texttt{send()} / \texttt{recv()} 接口与事件通知语义。
图 ~\ref{fig:nvlink-executor} 展示了 NVLink 链路执行器的核心类结构与成员变量。

\begin{figure}[htbp]
    \centering
    \begin{minted}[
        frame=lines,
        linenos,
        fontsize=\small,
        numbersep=10pt,
        framesep=3mm
    ]{cpp}
class NvlinkSender : public Sender, public Pollable {
// ...
private:
  ipc::SPSCQueue<NvlinkSendTicket> send_ticket_queue_;
  ipc::SPSCQueue<NvlinkAck> recv_ack_queue_;
  HandleCache handle_cache_;
  MultiMap<shared_ptr<Event>> pending_send_event_map_;
  shared_ptr<Event> inner_send(uint32_t unique_id, uint64_t offset,
                               uint64_t length,
                               cudaIpcMemHandle_t handle);
public:
  shared_ptr<Event> send(uint32_t unique_id, uint64_t addr,
                         uint64_t length) override {
    return this->inner_send(/*...*/);
  }
  void poll() override noexcept;
};

class NvlinkRecver : public Recver, public Pollable {
// ...
private:
  ipc::SPSCQueue<NvlinkSendTicket> recv_ticket_queue_;
  ipc::SPSCQueue<NvlinkAck> send_ack_queue_;
  Queue<NvlinkRecvCommand> recv_request_command_queue_;
  MultiMap<NvlinkSendTicket> pending_remote_send_map_;
  MultiMap<NvlinkRecvCommand> pending_local_recv_map_;
  Queue<NvlinkCopyTask> copy_task_queue_;
  NvlinkCopier copier_;
};
    \end{minted}
    \caption{NVLink 链路执行器}
    \label{fig:nvlink-executor}
\end{figure}

\subsection{跨进程通信内存访问机制}

NVLink 链路围绕 CUDA IPC 提供的跨进程访问和 Peer Access功能构建\cite{nvidia2025cudipc,nvidia_nvlink_nvswitch}。
发送方在应用层调用 \texttt{send()} 时，
通过 \texttt{cudaIpcGetMemHandle()} 将本地 GPU 缓冲区封装为 \texttt{cudaIpcMemHandle\_t} 并传给接收方；
接收方在拿到该 handle 之后，首先调用 \texttt{cudaIpcOpenMemHandle()}
将发送端内存映射到本进程的地址空间，获取可以在接收方进程内部直接访问的指针，
然后就可以在接收方进程内部完成 GPU 到 GPU 的内存拷贝了。

与 RDMA 场景中基于 \texttt{RcQueuePair} 进行控制面消息传递不同的是，
NVLink 控制面依赖主机共享内存进行跨进程通信，在主机共享内存上构建单生产者单消费者（SPSC）队列，
用于在单节点的多个进程之间传递控制面信息 \texttt{NvlinkSendTicket} 和 \texttt{NvlinkAck}。
控制面信息的具体定义如图 ~\ref{fig:nvlink-control-plane-structure} 所示，
在 \texttt{NvlinkSendTicket} 中传递 offset 是因为接收端通过 \texttt{cudaIpcOpenMemHandle()} 获取到的指针
指向通过 \texttt{cudaIpcGetMemHandle()} 获得的设备共享内存的起始位置，
而实际的发送缓冲区并不确保从起始位置开始，需要 offset 进行计算。

在 \texttt{NvlinkSender} 中，\texttt{send\_ticket\_queue\_} 用作
发送端到接收端的单向控制面消息通道，
而 \texttt{recv\_ack\_queue\_} 则用于获取接收端回传的 \texttt{NvlinkAck}。
\texttt{NvlinkRecver} 中的成员则恰好承担反向角色。

由于局部指针跨进程传递无法直接解引用，SPSCQueue 要求元素为 POD 类型，控制面结构体不包含任何指针或复杂资源；
涉及 \texttt{std::shared\_ptr<Event>} 的 \texttt{NvlinkCopyTask} 仅在进程局部队列中流转，
不会经由共享内存 SPSC 队列跨进程传递，从而避免了跨进程管理 C++ 对象生命周期的复杂性。

\begin{figure}[htbp]
    \centering
    \begin{minted}[
        frame=lines,
        linenos,
        fontsize=\small,
        numbersep=10pt,
        framesep=3mm
    ]{cpp}
struct alignas(64) NvlinkSendTicket {
  cudaIpcMemHandle_t handle;
  uint64_t offset;
  uint32_t unique_id;
  uint64_t length;
};

struct alignas(8) NvlinkAck {
  uint32_t unique_id;
};
    \end{minted}
    \caption{NVLink 链路执行器控制面元数据}
    \label{fig:nvlink-control-plane-structure}
\end{figure}

\subsection{请求匹配与提交}

NVLink 执行器的请求的匹配与调度逻辑与 RDMA 链路的类似，同样采用 ``远端发送请求 + 本地接收请求'' 的双队列配对模式。
不同之处在于，NVLink 执行器采用``发送端发起''的模式，而请求匹配和调度的逻辑主要集中在接收端，

发送端首先调用 \texttt{send()} 发起通信请求，从本地缓存查找可用的 CUDA IPC Mem Handle，
如果不存在，则根据配置情况决定是否调用 \texttt{cudaIpcGetMemHandle()} 创建新的 handle 并缓存。
获取到 handle 之后，将相对 CUDA IPC Mem Handle 的起始地址偏移量、
长度以及逻辑通道标识符 \texttt{unique\_id} 打包为 \texttt{NvlinkSendTicket}，写入跨进程 SPSC 队列；
同时在 \texttt{pending\_send\_event\_map\_} 中插入一条待完成的 \texttt{Event}。

在接收端，\texttt{NvlinkRecver::poll()} 负责从对应的 SPSC 队列中取出 ticket 并进行匹配，
其流程如下：
\begin{enumerate}
    \item 从跨进程 SPSC 队列中取出远端 \texttt{NvlinkSendTicket}，
          按 \texttt{unique\_id} 插入 \texttt{pending\_remote\_send\_map\_}；
    \item 从本地 \texttt{recv\_request\_command\_queue\_} 中出队 \texttt{NvlinkRecvCommand}，
          按 \texttt{unique\_id} 插入 \texttt{pending\_local\_recv\_map\_}；
    \item 在上述两个映射上做配对，将匹配成功的二元组组装为 \texttt{NvlinkCopyTask}，
          放入 \texttt{copy\_task\_queue\_}，等待拷贝线程执行。
\end{enumerate}

在这一设计下 NVLink 接收端的控制面完全工作在 CPU 上，而所有 GPU 数据面操作则推迟到 Copier 的线程中执行。
事件语义与 RDMA 链路保持对齐，应用层始终在发送或接收请求对应的 \texttt{Event} 上等待，直到跨 GPU 内存拷贝完成后才被通知。

\subsection{数据面的实现}

\subsubsection{拷贝线程执行流程}

接收端链路执行器持有 \texttt{copy\_task\_queue\_} 的一端，并持续向其中注入拷贝任务；
拷贝线程从另一端取出拷贝任务并执行。

GPU 指针的跨进程可见性由 \texttt{NvlinkCopier} 管理，在创建 Copier 对象时，
构造函数在指定的 \texttt{device} 上创建 CUDA primary context，
并初始化一个专用的非阻塞 \texttt{cudaStream\_t}；
随后启动后台工作线程运行无限循环，循环的每一轮都尝试获取拷贝任务，并执行 GPU-to-GPU 拷贝。
其处理单个任务时流程如下：

\begin{enumerate}
    \item 调用 \texttt{cudaSetDevice(device\_)}，确保当前线程绑定到目标 GPU 的上下文；
    \item 使用 \texttt{cudaIpcOpenMemHandle()} 将 \texttt{remote\_ticket.handle} 映射为本进程可见的设备指针；
    \item 根据源设备映射后的远端指针和地址偏移量，计算出拷贝的源地址，
          在提前创建的非阻塞 stream 上调用 \texttt{cudaMemcpyAsync()} 将数据拷贝到本地目标地址；
    \item 通过 \texttt{cudaEventSynchronize()} 等待拷贝完成；
    \item 调用 \texttt{local\_event->notify()} 将结果回传给应用层，
          构造 \texttt{NvlinkAck} 通过 \texttt{send\_ack\_queue\_} 发送给 Sender；
    \item 调用 \texttt{cudaIpcCloseMemHandle()} 关闭远端指针映射。
\end{enumerate}

通过将 CUDA IPC handle 的管理逻辑集中在 Copier 内部，\sysname 将跨进程 GPU 指针管理复杂度屏蔽，
保证了 handle 生命周期与数据传输严格绑定。

\subsubsection{CUDA IPC Memory Handle 性能优化}

\paragraph{性能开销分析}
从操作系统层面看，\texttt{cudaIpcOpenMemHandle()} 并非简单的“反序列化指针”，
其主要开销来自于跨进程设备内存映射的建立过程：
运行时需要将发送端导出的设备内存对象引入到接收端进程的 CUDA context 中，
并为该内存建立可被本进程访问的虚拟地址映射关系。这一过程通常涉及：
\begin{enumerate}
    \item CUDA context 内部全局锁与元数据查找；
    \item 为映射区间创建或更新 GPU 虚拟地址空间与页表项；
    \item 按需启用 Peer Access；
    \item 映射/解除映射阶段可能触发的同步语义与资源回收。
\end{enumerate}
相对而言，\texttt{cudaMemcpyAsync()} 的开销主要与数据量相关，
而打开/关闭的固定成本更高，因此在小消息场景下尤为显著。

如图 ~\ref{fig:cuda-ipc-overhead} 所示，打开/关闭 CUDA IPC Handle 的延迟极高，
并且随着被映射内存区域增大呈上升趋势：当映射的 buffer 足够大时，单次打开/关闭甚至达到毫秒级，
这会直接限制 \sysname 在 NVLink 数据面的端到端吞吐，造成显著的 GPU 地址空间管理压力。

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/cuda_ipc_open_close_overhead_vs_size.png}
    \caption{CUDA IPC Open/Close 开销}
    \label{fig:cuda-ipc-overhead}
\end{figure}

\paragraph{缓存复用的优化思路}
针对上述瓶颈，\sysname 在数据面引入IPC 映射缓存：
对于同一个发送端导出的 \texttt{cudaIpcMemHandle\_t}，接收端不再在每个任务中重复打开/关闭，
而是在首次遇到该 handle 时执行一次 \texttt{cudaIpcOpenMemHandle()}，
将得到的远程内存基地址缓存起来，后续所有属于该 buffer 的拷贝任务直接复用该映射指针；
后续发送端如果不再使用该内存区域，则通过控制面通知接收端关闭该 handle 并从缓存中移除。
这样可以将打开/关闭的摊销成本从“每次拷贝一次”降低为“每个 buffer 生命周期一次”，
从而显著提升性能。

\section{本章小结}
本章从实现角度给出了 \sysname 的关键模块与工程细节。
首先介绍了应用接口层的统一抽象，并给出了运行时的线程池轮询框架与可驱动对象接口，
说明其如何为不同链路执行器提供一致的调度环境与线程上下文隔离；
随后给出了基于 C++20 原子等待/通知实现的事件机制，用以支持低开销的异步完成语义。
在链路执行器部分，本章详细展开了 RDMA 执行器的资源初始化、控制面缓冲区与 MR 注册、批处理发送与接收路径，
并通过 \texttt{RdmaFlusher} 进一步保证 GPU Direct RDMA 场景下的数据可见性与一致完成语义；
同时，本章给出了 NVLink 执行器在节点内跨进程场景下的控制面队列组织、后台拷贝线程推进方式，
以及通过 CUDA IPC 映射缓存复用降低 open/close 开销的关键优化。
通过上述实现，\sysname 在保持统一接口的同时实现了面向 RDMA 与 NVLink 的高效、可扩展且 SM-free 的通信控制面与数据面协同。
