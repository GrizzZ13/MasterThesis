% !TEX root = ../main.tex

\chapter{绪论}

\section{研究背景}

近年来，以大语言模型（Large Language Model, LLM）为代表的新兴人工智能技术飞速发展，正深刻重塑科技产业格局。LLM 凭借强大的自然语言理解与生成能力，在代码生成、内容创作、知识问答乃至复杂推理等任务中展现出前所未有的性能。缩放定律（Scaling Law）是这一技术进步的核心驱动力：大量实证研究表明，模型性能随参数量、训练数据量以及计算量的增加而持续提升，呈现出可预测的幂律关系。这一规律不仅指导了模型架构与训练策略的设计，也推动了基础设施层面的系统性演进。

受 Scaling Law 驱动，模型参数量呈指数级增长。早期模型如 BERT（约 0.34B 参数）和 GPT-2（1.5B）尚可在单卡 GPU 上完成推理；而如今主流开源模型已普遍迈入百亿乃至千亿量级——例如 DeepSeek-R1-671B 包含 6710 亿参数，Qwen3-235B 也达到 2350 亿参数规模。如此庞大的参数体量使得模型无法被单个 GPU 的高带宽内存（High-Bandwidth Memory, HBM）所容纳。以 FP16 精度部署 Qwen3-235B 即需约 470 GB 显存，远超当前顶级消费级（如 NVIDIA RTX 4090，24 GB）乃至数据中心级 GPU（如 H20，140 GB）的内存容量。展望未来，行业级模型参数量有望突破万亿级别，对推理系统的内存与通信能力提出更高要求。

为部署如此大规模的模型，分布式推理已成为必要手段。当前主流的并行策略主要包括张量并行（Tensor Parallelism, TP）、流水线并行（Pipeline Parallelism, PP）、数据并行（Data Parallelism, DP）以及专家并行（Expert Parallelism, EP）。TP 将注意力头（Attention Head）切分到多个设备上协同计算，并引入集合通信规约（AllReduce）不同注意力头的结果，通信密集但可降低单设备的计算和内存压力；PP 将模型按层拆分至不同设备，通过流水线重叠计算与通信；DP 复制完整模型副本，不同请求路由到不同模型副本进行推理，提高了系统的并行度；EP 则专为稀疏激活的混合专家（Mixture of Experts, MoE）架构设计，仅路由激活部分专家，需高效通信机制支持。这些并行策略常组合使用，以在计算、内存与通信之间取得平衡。然而，无论采用何种并行方式，设备间频繁的数据交换都高度依赖底层通信库的性能，使其成为制约大模型推理吞吐与延迟的关键瓶颈。

随着推理系统的不断演进，又进一步演进出了预填充-解码分离（Prefill-Decode Disaggregaton）推理架构，该架构将推理流程分为计算密集型的预填充（Prefill）阶段和内存密集型的解码（Decode）阶段，并将它们分离到不同的硬件资源上，在预填充完成后，需要将产生的键值缓存（KV Cache）从预填充节点传输到解码节点，从而实现对两个阶段的独立优化。PD 分离系统相比于 PD 混合（PD-Colocation）系统降低了首个令牌时间（Time-to-first-token, TTFT），提升了系统的令牌吞吐（Token Throughput）。然而，PD分离中 KV Cache 的跨界点迁移对于网络传输性能提出了更高的要求，尤其依赖于高带宽、低时延的通信机制。

此外，在模型即服务(Model-as-a-Service，MaaS) 的云原生部署环境中，云服务商为了提高资源利用率，可能会采用模型按需部署的方式，对于模型推理服务实例根据用户请求用量进行自动扩缩容，从而降低成本。为了满足服务等级目标（Service Level Objective），在进行模型扩容时的关键点在于加速模型推理实例的启动速度，其中模型参数加载的时间占据了实例启动时间的主要部分。为了加速模型参数加载速度，云服务商采用通过网络进行参数传输的优化方式，将模型参数从一些正在运行的 GPU 实例传输到空闲的 GPU 实例上，极大降低了模型推理服务的启动时间。在通过网络加载参数的过程中，由于模型参数规模较大，涉及到大量节点内、节点见的网络通信，对通信库的性能提出了更高的要求。

为了应对大模型推理场景中的各种不同的通信需求，学术界和工业界提出了多种不同的 GPU 通信方案，包括 NCCL、NVSHMEM、UCCL-P2P、Mooncake 等。在大模型推理系统中，通信库一方面需要对开发者提供易用的编程接口，另一方面需要针对不同的硬件拓扑和通信模式进行优化，以最大化通信性能。

目前主流的 GPU 通信库存在如下问题，影响大模型推理系统整体性能的进一步提升：

\begin{itemize}
  \item 数据传输控制面与计算任务存在资源竞争。GPU 通信任务的控制面通常依赖于流式多处理器（Streaming Multiprocessor, SM），通信任务与计算任务存在 SM 资源争抢，影响计算效率，导致推理系统的整体性能下降。
  \item 现有通信库不能很好地应对推理系统流量的复杂性。在大模型推理系统中，通常存在多种不同的通信模式，从而引入了不同的流量类型。多种流量之间可能存在带宽竞争，当带宽有限时，如果一个带宽敏感的通信任务被其他大流量通信任务挤占带宽，可能会导致通信延迟大幅增加，进而影响推理系统的整体性能。
\end{itemize}

如果通信库能够进行控制面卸载（Control Plane Offloading），避免通信任务控制面与计算任务的资源竞争，并且在数据传输时充分考虑到流量的优先级问题，那么能够进一步优化大模型推理系统的整体性能。

\section{研究现状}

NCCL（NVIDIA Collective Communication Library）是由 NVIDIA 推出的高性能通信库，主要面向多 GPU 与多节点环境下的深度学习训练与高性能计算任务。其核心目标是为分布式系统提供高效、可扩展的集合通信（Collective Communication）原语，包括 All-Reduce、All-Gather、Reduce、Broadcast 等操作，从而降低通信开销、提升分布式训练效率。NCCL 针对 NVIDIA GPU 架构进行了深度优化，充分利用 GPU 之间的高速互联技术（如 NVLink）、网络互联技术（如 RDMA、GPU Direct RDMA） ，实现数据在 GPU 内存之间的直接传输，显著减少冗余数据拷贝带来的性能损失。此外，NCCL 使用拓扑感知的通信算法，通过自动检测系统拓扑结构构建最优通信路径，确保在不同硬件配置下均能获得接近理论峰值的通信带宽。作为主流深度学习框架（如 PyTorch 和 TensorFlow）的底层通信模块，NCCL 已成为 GPU 集群环境中高性能集合通信的事实标准，在大规模模型训练、推理场景中发挥着关键作用。然而 NCCL 在进行跨节点 RDMA 通信时，采用代理线程模式，由 CPU 和 GPU 共享信号缓冲区（Signal buffer），在 CPU 上启动网卡轮询线程监测通信任务执行情况，并将结果写会信号缓冲区，然后 GPU 端通过轮询信号缓冲区获取通信任务执行结果，从而进行网卡-CPU-GPU的跨设备同步。这种代理线程、多级轮询、跨设备同步的设计，导致 NCCL 在进行跨节点通信时引入额外延迟，不能应对延迟敏感场景工作负载的需求。而且 NCCL 在进行节点内通过 NVLink 的多 GPU 通信时，启动大量 GPU 线程进行 Load/Store 操作，消耗 SM 资源，会和计算任务产生资源竞争，影响计算效率。

NVSHMEM 是由 NVIDIA 公司开发的一种面向 GPU 的、基于 OpenSHMEM 标准的单边通信库，旨在为大规模异构系统中的 GPU 间通信提供高效、可扩展的编程抽象。其设计目标是支持细粒度、低延迟的远程内存访问，在以 GPU 为中心、无 CPU 参与情况下，实现多 GPU 之间的直接数据交换。NVSHMEM 遵循 Partitioned Global Address Space（PGAS）编程模型，为每个PE（Processing Element）提供一个全局可寻址的对称内存空间，通过提供如 Put、Get 等单边通信原语，NVSHMEM 允许一个 PE 直接读写远程 PE 的全局内存区域。在底层实现上，NVSHMEM 依赖于统一通信抽象（UCX）框架，可自适应利用 NVLink 或 RDMA 等高速互连技术，以优化带宽与延迟。并且 NVSHMEM 在进行跨节点 RDMA 通信时引入了 IBGDA （InfiniBand GPUDirect Async）的能力，使用 GPU SM 直接访问网卡 Doorbell 寄存器，消除了 CPU 代理线程的参与，显著优化了小消息通信时的性能。相较于 NCCL，NVSHMEM 更加专注于单边通信，适用于需要频繁、细粒度数据交换的应用场景。但是 NVSHMEM 仍然依赖 GPU SM 驱动数据传输，在 P2P 通信的场景传输大量的数据时抢占计算资源，影响计算任务的性能。

UCCL-P2P 是加州大学伯克利分校的 Sky Computing Lab 提出的高性能 KV Cache 传输引擎，提供了 SM-free 的特性，轻量级的项目代码，以及易用性强的编程接口。UCCL-P2P 使用 GPU Direct RDMA 技术实现高性能的跨节点 KV Cache 传输，与 NCCL 不同的是 UCCL-P2P通过控制面卸载（Control Plane Offloading）技术，完全消除了 GPU SM 在数据传输过程中的参与，避免了通信任务与计算任务的资源竞争，从而提升了大模型推理系统的整体性能。但是 UCCL-P2P 目前仅支持 SM-free 的 RDMA 通信模式，目前不支持节点内多 GPU 之间通过 NVLink 进行 SM-free 的高带宽通信，存在一定局限性，限制了其在大模型推理系统中的应用场景。

\section{主要研究内容}

本文针对大模型推理系统中通信库目前所面临的数据传输控制面与计算任务存在资源竞争、不能很好应对推理系统流量的复杂性的问题，提出并实现了大模型系统通信库\sysname。\sysname 的设计目标是通过控制面卸载（Control Plane Offloading）技术，将驱动数据传输的控制面逻辑完全卸载到 CPU 上，避免通信任务控制面与计算任务的资源竞争，并且在数据传输时充分考虑到流量的优先级问题，从而优化大模型推理系统的整体性能。

在实现跨节点通信时，\sysname 使用 GPU Direct RDMA 技术，将数据从一个 GPU 直接搬运到另一个节点的 GPU 内存中，避免了冗余的数据拷贝操作，并且通过 CPU 进行网卡轮询操作。在实现节点内通信时，\sysname 利用基于 Cuda 进程间通信 （Cuda IPC）的 cudaMemcpy，通过 GPU 的拷贝引擎（Copy Engine，CE）避免了直接 Load/Store 带来的 SM 资源占用。

\sysname 实现了异步任务运行时，通过后台工作线程（Worker Thread）处理通信任务的控制面逻辑，驱动数据面的执行。并且实现了基于优先级且支持抢占的任务调度器，能够根据不同通信任务的优先级动态调整带宽分配策略，从而提升延迟敏感任务的性能。\sysname 通过维护任务队列，通过入队函数和出队函数的接口和运行时交互，实现任务的调度。

基于控制面卸载技术和优先级任务调度器，使用 C++ 语言实现了 \sysname 通信库。由于采用了控制面卸载，\sysname 避免了上述通信库提到的问题。\todo{再补充一些}

\section{论文组织架构}

本文由绪论、相关技术背景、\sysname 系统架构与设计、\sysname 系统实现、实验和评测、全文总结六个章节组成。
第一章绪论从研究背景、研究现状、主要研究内容几个方面概括介绍了本文的研究内容。
第二章相关技术背景详细介绍了本文所涉及的大模型推理系统以及 RDMA 和 GPU 通信的相关技术背景。
第三章 \sysname 系统架构与设计首先介绍了 \sysname 系统的总体架构，然后 \todo{}。
第四章 \sysname 系统实现介绍了 \sysname 系统的实现细节，包括用于性能优化的设计，以及具体的编程接口。
第五章包含对 \sysname 系统的实验和评测，通过实验验证了 \sysname 系统的性能优化效果。
第六章对本文的研究内容进行了总结，同时讨论了未来工作的展望。
